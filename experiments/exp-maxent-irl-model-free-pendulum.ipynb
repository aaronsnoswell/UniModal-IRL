{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pretty-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import gym\n",
    "import cma\n",
    "import math\n",
    "import torch\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from scipy.optimize import minimize\n",
    "from joblib import Parallel, delayed\n",
    "from IPython.display import display\n",
    "\n",
    "from gym.wrappers import TimeLimit\n",
    "from gym.envs.classic_control import PendulumEnv\n",
    "\n",
    "from stable_baselines3 import DDPG, A2C, PPO, TD3\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "from mdp_extras import vi, OptimalPolicy, padding_trick, UniformRandomCtsPolicy, PaddedMDPWarning, Linear\n",
    "from mdp_extras.envs import (\n",
    "    CustomPendulumEnv,\n",
    "    VonMisesNormalBasis,\n",
    "    pendulum_obs_to_state\n",
    ")\n",
    "\n",
    "from unimodal_irl import sw_maxent_irl, sw_maxent_irl_modelfree, mean_ci, ile_evd, inner_angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acoustic-lambda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snoswell\\Miniconda3\\envs\\py38\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#env = gym.make(\"Pendulum-v0\")\n",
    "max_timesteps = 200\n",
    "env = PendulumEnv()\n",
    "env = TimeLimit(env, max_timesteps)\n",
    "gamma = 0.99\n",
    "\n",
    "# Load trained DDPG policy from rl-baselines3-zoo\n",
    "model = DDPG.load(\n",
    "    os.path.join(\"rl-trained-agents/ddpg/Pendulum-v0_1/Pendulum-v0.zip\"),\n",
    "    env=env,\n",
    "    custom_objects=dict(learning_rate=0.001)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7b53d9-16d3-45d6-b6b1-7190f4039174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preview_policy(policy, num_eps=2):\n",
    "    for _ in range(num_eps):\n",
    "        gain = 0\n",
    "        obs = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            a, _ = policy.predict(obs)\n",
    "            obs, r, done, info = env.step(a)\n",
    "            env.render()\n",
    "            gain += r\n",
    "            if done:\n",
    "                break\n",
    "        env.close()\n",
    "        print(\"Gain = \", gain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "palestinian-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect demonstrations from expert policy\n",
    "num_demos = 20\n",
    "demos = []\n",
    "for _ in range(num_demos):\n",
    "    obs = env.reset()\n",
    "    traj = []\n",
    "    while True:\n",
    "        state = pendulum_obs_to_state(obs)\n",
    "        action, _ = model.predict(obs)\n",
    "        traj.append((state, action))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        #env.render()\n",
    "        if done:\n",
    "            break\n",
    "    demos.append(traj)\n",
    "    #env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acute-evans",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.37228931e-04 2.60523678e-01 2.81397210e+01 1.74939481e-01\n",
      "  3.41200528e-05]\n",
      " [2.31296580e-01 6.76056018e-01 2.59988624e-01 1.51151544e-01\n",
      "  8.23808584e-03]\n",
      " [5.44339201e-01 1.68049880e-01 3.72317137e-01 2.68556664e-01\n",
      "  1.39230871e-01]\n",
      " [3.83788671e-01 3.49575355e-01 4.83818746e-01 4.43847770e-01\n",
      "  1.64711023e-01]\n",
      " [1.14475265e-02 3.09312827e-01 3.71189662e-01 3.37865406e-01\n",
      "  7.78023274e-02]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "basis_dim = 5\n",
    "phi = VonMisesNormalBasis(num=basis_dim, kappa=10, std=1.0)\n",
    "\n",
    "# Visualise feature function\n",
    "# num = 20\n",
    "# t = np.linspace(-np.pi, np.pi, num)\n",
    "# td = np.linspace(-8.0, 8.0, num)\n",
    "\n",
    "# X, Y = np.meshgrid(t, td)\n",
    "# Z = np.zeros([X.shape[0], X.shape[1], basis_dim ** 2])\n",
    "# for id0 in range(X.shape[0]):\n",
    "#     for id1 in range(X.shape[1]):\n",
    "#         _x = X[id0, id1]\n",
    "#         _y = Y[id0, id1]\n",
    "#         Z[id0, id1] = phi([_x, _y])\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# for _idx in range(basis_dim ** 2):\n",
    "#     plt.figure()\n",
    "#     _Z = Z[:, :, _idx]\n",
    "#     plt.contour(X, Y, _Z)\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "\n",
    "phi_bar = phi.demo_average(demos, gamma)\n",
    "\n",
    "print(phi_bar.reshape(basis_dim, -1))\n",
    "\n",
    "def viz_soln(x, cmap=\"RdBu\"):\n",
    "    \"\"\"Visualize a solution vector\"\"\"\n",
    "    #plt.figure(figsize=(7, 6), dpi=300)\n",
    "    plt.figure()\n",
    "    sns.set()\n",
    "    plt.imshow(x.reshape(basis_dim, -1), aspect=\"auto\", extent=(-np.pi, np.pi, -8.0, 8.0))\n",
    "    plt.set_cmap(cmap)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize expert's feature expectation\n",
    "viz_soln(phi_bar, \"viridis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcefdfe-6c12-42a5-b7c9-ac31b948e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a very fast approximate feature lookup table\n",
    "t_bounds = np.array([-np.pi, np.pi])\n",
    "t_delta = t_bounds[1] - t_bounds[0]\n",
    "td_bounds = np.array([-8, 8])\n",
    "td_delta = td_bounds[1] - td_bounds[0]\n",
    "\n",
    "num_disc = 1000\n",
    "\n",
    "def s2int(s):\n",
    "    t, td = s\n",
    "    t_idx = int(np.round((t - t_bounds[0]) / t_delta * (num_disc - 1)))\n",
    "    td_idx = int(np.round((td - td_bounds[0]) / td_delta * (num_disc - 1)))\n",
    "    ttd_idx = td_idx + num_disc * t_idx\n",
    "    ttd_idx = min(max(ttd_idx, 0), (num_disc * num_disc) - 1)\n",
    "    return ttd_idx\n",
    "\n",
    "# # Sweep state space\n",
    "# thetas = np.linspace(*t_bounds, num_disc, endpoint=False)\n",
    "# theta_dots = np.linspace(*td_bounds, num_disc)\n",
    "# state_sweep = [np.array(p) for p in it.product(thetas, theta_dots)]\n",
    "\n",
    "# print(\"Building feature LUT\", flush=True)\n",
    "# phi_lut = []\n",
    "# for s in tqdm(state_sweep):\n",
    "#     phi_lut.append(phi(s))\n",
    "\n",
    "# # Save to disk\n",
    "# with open(\"phi-s-lut.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(phi_lut, file)\n",
    "#del phi_lut\n",
    "\n",
    "# Load from disk\n",
    "with open(\"phi-s-lut.pkl\", \"rb\") as file:\n",
    "    phi_lut = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a807dae-07b3-402b-a19a-918bd32b9aba",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Value function is given by:\n",
    "\n",
    "$$\n",
    "v_\\pi(s) \\triangleq \\mathbb{E}_\\pi\\left[\n",
    "    \\sum_{k=0}^\\infty \\gamma^k r(s_{t+k+1}) \\mid s_t = s\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Therfore, for a given policy we can approximate this by,\n",
    "\n",
    "$$\n",
    "v_\\pi(s) \\approx \\frac{1}{|M|} \\sum_{i=1}^M\n",
    "\\left[\n",
    "    r(s) + r(s') + \\dots \\mid T, \\pi\n",
    "\\right];\n",
    "\\qquad\\qquad \\tau_i \\sim \\pi\n",
    "$$\n",
    "\n",
    "This approximation will have much lower variance for a deterministic policy, and will be exact up to numerical rounding for the case of a deterministic policy AND detemrinistic dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14cf2927-7b09-412e-b157-9aaad1181bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def approx_state_value(model, s, num_samples=1, gamma=1.0, r_custom=None):\n",
    "    \"\"\"Approximately compute the value of s under pi\n",
    "    \n",
    "    Args:\n",
    "        pi (class): Policy object with a .predict() method matching the stable-baselines API\n",
    "        s (numpy array): State to estimate value from\n",
    "        \n",
    "        num_samples (int): Number of samples to estimate value with. For\n",
    "            determinstic policies and transition dynamics this can be set to 1.\n",
    "        gamma (float): Discount factor\n",
    "        r_custom (mdp_extras.RewardFunction): Custom reward function to use\n",
    "    \n",
    "    Returns:\n",
    "        (float): State value estimate\n",
    "    \"\"\"\n",
    "    pi = model\n",
    "    \n",
    "    episode_returns = []\n",
    "    for _ in range(num_samples):\n",
    "        # XXX Force initial state\n",
    "        env.reset()\n",
    "        env.unwrapped.state = s\n",
    "        obs = env.unwrapped._get_obs()\n",
    "        done = False\n",
    "        ep_rewards = []\n",
    "        while not done:\n",
    "            a = pi.predict(obs, deterministic=True)[0]\n",
    "            obs, reward, done, info = env.step(a)\n",
    "            if r_custom is not None:\n",
    "                # Use custom reward function\n",
    "                state = pendulum_obs_to_state(obs)\n",
    "                reward = r_custom(phi(state))\n",
    "            ep_rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        ep_rewards = np.array(ep_rewards)\n",
    "        gammas = gamma ** np.arange(len(ep_rewards))\n",
    "        episode_return = gammas @ ep_rewards\n",
    "        episode_returns.append(episode_return)\n",
    "    \n",
    "    return np.mean(episode_returns)\n",
    "\n",
    "\n",
    "def approx_policy_value(model, start_state_disc_dim=10, num_samples=1, gamma=1.0, r_custom=None, n_jobs=8):\n",
    "    \"\"\"Approximately compute the value pi under the starting state distribution\n",
    "    \n",
    "    Args:\n",
    "        pi (class): Policy object with a .predict() method matching the stable-baselines API\n",
    "    \n",
    "        start_state_disc_dim (int): How fine to discretize each dimension of the MDP starting\n",
    "            state distribution support. For Pundulum-v0, 10 seems to be sufficient for accurately\n",
    "            measuring policy value (at least for the optimal policy)\n",
    "        num_samples (int): Number of samples to estimate value with. For\n",
    "            determinstic policies and transition dynamics this can be set to 1.\n",
    "        gamma (float): Discount factor\n",
    "        r_custom (mdp_extras.RewardFunction): Custom reward function to use\n",
    "        n_jobs (int): Number of parallel workers to spin up for estimating value\n",
    "        \n",
    "    Returns:\n",
    "        (float): Approximate value of pi under the MDP's start state distribution\n",
    "    \"\"\"\n",
    "    # Compute a set of states that span and discretize the continuous uniform start state distribution\n",
    "    theta_bounds = np.array([-np.pi, np.pi])\n",
    "    theta_delta = 0.5 * (theta_bounds[1] - theta_bounds[0]) / start_state_disc_dim\n",
    "    theta_bounds += np.array([theta_delta, -theta_delta])\n",
    "    thetas = np.linspace(theta_bounds[0], theta_bounds[1], start_state_disc_dim)\n",
    "    theta_dots = np.linspace(-1, 1, start_state_disc_dim)\n",
    "    start_states = [np.array(p) for p in it.product(thetas, theta_dots)]\n",
    "    \n",
    "    # Spin up a bunch of workers to process the starting states in parallel\n",
    "    values = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(approx_state_value)(model, state, num_samples, gamma, r_custom)\n",
    "        for state in start_states\n",
    "    )\n",
    "    return np.mean(values)\n",
    "\n",
    "# What is the value of the optimal policy?\n",
    "#pi_gt_v = approx_policy_value(model)\n",
    "#print(pi_gt_v)\n",
    "# -144 is *just* sufficient to make it to the OpenAI Gym leaderboard - so we're in the right ball-park\n",
    "\n",
    "# Best value from OpenAI Leaderboard: https://github.com/openai/gym/wiki/Leaderboard#pendulum-v0\n",
    "pi_gt_v = -123.11\n",
    "\n",
    "\n",
    "def evd(model, gamma, n_jobs=8):\n",
    "    \"\"\"Compute approximate expected value difference for a learned optimal policy\n",
    "    \n",
    "    Args:\n",
    "        learned_td3_model_path (str): Filename for saved model parameters\n",
    "        \n",
    "        gamma (float): Discount factor\n",
    "        \n",
    "    Returns:\n",
    "        (float): Expected value difference of the given policy\n",
    "    \"\"\"\n",
    "    v_pi = approx_policy_value(model, gamma=gamma, n_jobs=n_jobs)\n",
    "    evd = pi_gt_v - v_pi\n",
    "    return evd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c7806be-d060-4894-a821-b182c43b786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def approx_opt_pol(theta, mdl_fname=\"mdl.td3\"):\n",
    "    \"\"\"Compute an approximate optimal policy for a given Pendulum linear reward parameter vector\n",
    "    \n",
    "    Args:\n",
    "        theta (numpy array): Reward parameter vector\n",
    "        \n",
    "        mdl_fname (str): String where to save model to\n",
    "    \n",
    "    Returns:\n",
    "        (float): Expected Value Difference of the policy before training\n",
    "        (torch.model): Trained policy on CPU device, ready for evaluating\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build tmp environment for training\n",
    "    mdl = TD3(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=0,\n",
    "        tensorboard_log=\"./tb-log/\",\n",
    "        # Initially load on CPU so we can evaluate quickly\n",
    "        device=\"cpu\",\n",
    "\n",
    "        # Non-standard params from rl-baselines3-zoo\n",
    "        # https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/td3.yml\n",
    "        policy_kwargs=dict(net_arch=[400, 300]),\n",
    "        action_noise=NormalActionNoise(0, 0.1),\n",
    "        learning_starts=10000,\n",
    "        buffer_size=200000,\n",
    "        gamma=gamma\n",
    "    )\n",
    "    print(\"PRE-training preview\")\n",
    "    preview_policy(mdl)\n",
    "    mdl.save(mdl_fname)\n",
    "    pre_train_evd = evd(mdl, gamma, n_jobs=1)\n",
    "    \n",
    "    # Re-load to get onto GPU for training\n",
    "    custom_reward = lambda s: theta @ phi_lut[s2int(s)]\n",
    "    _env = Monitor(TimeLimit(CustomPendulumEnv(reward_fn=custom_reward), max_timesteps), filename=\"pendulum-log\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    mdl_gpu = TD3.load(mdl_fname, env=_env, device=device)\n",
    "    mdl_gpu.learn(\n",
    "        total_timesteps=1e5,\n",
    "        log_interval=5\n",
    "    )\n",
    "    mdl_gpu.save(mdl_fname)\n",
    "    \n",
    "    mdl_out = TD3.load(mdl_fname, env=env, device=\"cpu\")\n",
    "    print(\"POST-training preview\")\n",
    "    preview_policy(mdl_out)\n",
    "    post_train_evd = evd(mdl_out, gamma, n_jobs=1)\n",
    "    \n",
    "    return pre_train_evd, mdl_out, post_train_evd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-workstation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up to 1024-0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "experimental_data = []\n",
    "num_repeats = 10\n",
    "\n",
    "m_vals = 2 ** np.arange(10 + 1)\n",
    "for num_sampled_paths in m_vals:\n",
    "\n",
    "    # Initial point\n",
    "    x0 = np.zeros(len(phi))\n",
    "\n",
    "    # Bounds\n",
    "    bounds = [[-1.0, 1.0] for _ in range(len(phi))]\n",
    "    \n",
    "    # Importance sampling policy\n",
    "    pi_ref = UniformRandomCtsPolicy((-2.0, 2.0))\n",
    "    \n",
    "    for repeat in range(num_repeats):\n",
    "        \n",
    "        print(f\"Up to {num_sampled_paths}-{repeat}\")\n",
    "\n",
    "        # Get importance sampling dataset\n",
    "        pi_ref_demos = []\n",
    "        max_path_length = max_timesteps\n",
    "        for _ in range(num_sampled_paths):\n",
    "            path_len = np.random.randint(1, high=max_path_length + 1)\n",
    "            path = []\n",
    "            obs = env.reset()\n",
    "            s = pendulum_obs_to_state(obs)\n",
    "            while len(path) < path_len - 1:\n",
    "                a = pi_ref.predict(s)[0]\n",
    "                path.append((s, a))\n",
    "                obs, r, done, _ = env.step(a)\n",
    "                s = pendulum_obs_to_state(obs)\n",
    "            path.append((s, None))\n",
    "            pi_ref_demos.append(path)\n",
    "\n",
    "        # Pre-compute sampled path feature expectations\n",
    "        pi_ref_demo_phis_precomputed = [\n",
    "            phi.onpath(p, gamma)\n",
    "            for p in pi_ref_demos\n",
    "        ]\n",
    "\n",
    "        # Solve with biased gradient\n",
    "        res = minimize(\n",
    "            sw_maxent_irl_modelfree,\n",
    "            x0,\n",
    "            args=(gamma, phi, phi_bar, max_path_length, pi_ref, pi_ref_demos, False, pi_ref_demo_phis_precomputed),\n",
    "            method='L-BFGS-B',\n",
    "            jac=True,\n",
    "            bounds=bounds,\n",
    "            options=dict(disp=True)\n",
    "        )\n",
    "        theta = res.x\n",
    "        pre_evd, mdl, post_evd = approx_opt_pol(theta)\n",
    "        experimental_data.append([\n",
    "            # Importance sampling policy\n",
    "            \"Uniform\",\n",
    "            # How many sampled paths?\n",
    "            num_sampled_paths,\n",
    "            # Optimizer and gradient type\n",
    "            \"L-BFGS-B-is\",\n",
    "            # EVD before and after training\n",
    "            pre_evd,\n",
    "            post_evd,\n",
    "            # Learned reward\n",
    "            theta,\n",
    "            # Optimizer result/information\n",
    "            res\n",
    "        ])\n",
    "\n",
    "        # Nelder Mead doesn't work - the scipy implementation doesn't support bounds or callback termination signals\n",
    "        # See: https://github.com/scipy/scipy/issues/9412\n",
    "\n",
    "        # Solve with L-BFGS-B two-point\n",
    "        res = minimize(\n",
    "            sw_maxent_irl_modelfree,\n",
    "            x0,\n",
    "            args=(gamma, phi, phi_bar, max_path_length, pi_ref, pi_ref_demos, True, pi_ref_demo_phis_precomputed),\n",
    "            method='L-BFGS-B',\n",
    "            jac='2-point',\n",
    "            bounds=bounds,\n",
    "            options=dict(disp=True)\n",
    "        )\n",
    "        theta = res.x\n",
    "        pre_evd, mdl, post_evd = approx_opt_pol(theta)\n",
    "        experimental_data.append([\n",
    "            # Importance sampling policy\n",
    "            \"Uniform\",\n",
    "            # How many sampled paths?\n",
    "            num_sampled_paths,\n",
    "            # Optimizer and gradient type\n",
    "            \"L-BFGS-B-2point\",\n",
    "            # EVD before and after training\n",
    "            pre_evd,\n",
    "            post_evd,\n",
    "            # Learned reward\n",
    "            theta,\n",
    "            # Optimizer result/information\n",
    "            res\n",
    "        ])\n",
    "\n",
    "        # Solve with CMA\n",
    "        theta, es = cma.fmin2(\n",
    "            sw_maxent_irl_modelfree,\n",
    "            x0,\n",
    "            0.5,\n",
    "            args=(gamma, phi, phi_bar, max_path_length, pi_ref, pi_ref_demos, True, pi_ref_demo_phis_precomputed),\n",
    "            options=dict(bounds=bounds[0])\n",
    "        )\n",
    "        pre_evd, mdl, post_evd = approx_opt_pol(theta)\n",
    "        experimental_data.append([\n",
    "            # Importance sampling policy\n",
    "            \"Uniform\",\n",
    "            # How many sampled paths?\n",
    "            num_sampled_paths,\n",
    "            # Optimizer and gradient type\n",
    "            \"CMA-ES\",\n",
    "            # EVD before and after training\n",
    "            pre_evd,\n",
    "            post_evd,\n",
    "            # Learned reward\n",
    "            theta,\n",
    "            # Optimizer result/information\n",
    "            None\n",
    "        ])\n",
    "        \n",
    "        # Save checkpoint\n",
    "        with open(\"out.pkl\", \"wb\") as file:\n",
    "            pickle.dump(experimental_data, file)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    experimental_data,\n",
    "    columns=[\n",
    "        \"IS Policy\",\n",
    "        \"IS Budget\",\n",
    "        \"Optimizer\",\n",
    "        \"EVD Pre\",\n",
    "        \"EVD Post\",\n",
    "        \"Theta\",\n",
    "        \"Info\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "df.to_pickle(\"experimental_data.df.pkl\")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a35df-2845-410a-baff-d65bc9144100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
